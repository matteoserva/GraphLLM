###

# you can insert multiple clients in client_name.
# If one of them fails, GraphLLM will try the next one

client_name: [llama_cpp, openai]

###
llama_cpp:
    type: llama_cpp
    host: "localhost"
    port: 8080

    
    
# you can also select the model in the llm node by using {client: llama_swap:phi }
llama_swap:
    type: llama_swap
    base_url: http://localhost:8081
    model: qwen

openrouter:
    type: openai
    api_key: XXX
    base_url: https://openrouter.ai/api/v1
    model: meta-llama/llama-3.2-1b-instruct:free
    

groq:
    type: groq
    api_key: XXX

openai:
    type: openai
    api_key: XXX
    base_url: http://localhost:8080/v1
    
ollama:
    type: openai
    api_key: XXX
    base_url: http://localhost:11434/v1
