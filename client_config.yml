###

# you can insert multiple clients in client_name.
# If one of them fails, GraphLLM will try the next one

client_name: [llama_cpp]

###
llama_cpp:
    type: llama_cpp
    host: "localhost"
    port: 8080

groq:
    type: groq
    api_key: XXX

openai:
    type: openai
    api_key: XXX
    base_url: http://localhost:8080/v1

openrouter:
    type: openai
    api_key: XXX
    base_url: https://openrouter.ai/api/v1
    model: meta-llama/llama-3.2-1b-instruct:free